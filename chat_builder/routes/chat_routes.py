from flask import Blueprint, render_template, request, jsonify
import google.generativeai as genai
from groq import Groq
import os
from dotenv import load_dotenv

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©
load_dotenv()

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
models = {}

# ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Google Gemini Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ù…ØªÙˆÙØ±Ø§Ù‹
gemini_api_key = os.getenv('GOOGLE_API_KEY')
if gemini_api_key:
    try:
        genai.configure(api_key=gemini_api_key)
        models['gemini'] = genai.GenerativeModel('gemini-pro')
        print("Gemini model initialized successfully")
    except Exception as e:
        print(f"Error initializing Gemini model: {e}")

# ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Groq Llama Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ù…ØªÙˆÙØ±Ø§Ù‹
groq_api_key = os.getenv('GROQ_API_KEY')
if groq_api_key:
    try:
        models['llama'] = Groq(api_key=groq_api_key)
        print("Llama model initialized successfully")
    except Exception as e:
        print(f"Error initializing Llama model: {e}")

chat_bp = Blueprint('chat', __name__)

def get_chat_prompt(stage, text):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ø±Ø­Ù„Ø© ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    base_prompts = {
        'initial': "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ù†Ø­Ù† Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ù‚Ø¶ÙŠØªÙƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø®Ø¨Ø§Ø±Ù†Ø§ Ø£ÙˆÙ„Ø§Ù‹ Ø¹Ù† Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ù…Ù†Ø§Ù‚Ø´ØªÙ‡Ø§ØŸ",
        'parties': "Ø´ÙƒØ±Ø§Ù‹ Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©. Ø§Ù„Ø¢Ù†ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø®Ø¨Ø§Ø±Ù†Ø§ Ø¹Ù† Ø§Ù„Ø£Ø·Ø±Ø§Ù Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ©ØŸ\n\nÙ…Ø«Ù„Ø§Ù‹:\n- Ù…Ù† Ù‡Ù… Ø§Ù„Ø£Ø·Ø±Ø§Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠÙˆÙ†ØŸ\n- Ù…Ø§ Ù‡ÙŠ ØµÙØªÙ‡Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©ØŸ",
        'facts': "Ø­Ø³Ù†Ø§Ù‹ØŒ Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ø¯Ø« Ø¹Ù† ÙˆÙ‚Ø§Ø¦Ø¹ Ø§Ù„Ù‚Ø¶ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø³Ø±Ø¯ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø²Ù…Ù†ÙŠØŸ",
        'documents': "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ù„Ø¯ÙŠÙƒ Ù„Ø¯Ø¹Ù… Ø§Ù„Ù‚Ø¶ÙŠØ©ØŸ",
        'completion_check': "Ø¯Ø¹Ù†Ø§ Ù†Ù„Ø®Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ Ù‚Ø¯Ù…ØªÙ‡Ø§ Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†. Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© ØªÙˆØ¯ Ø¥Ø¶Ø§ÙØªÙ‡Ø§ØŸ"
    }
    
    prompt = base_prompts.get(stage, "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ")
    
    gemini_prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø²Ù…ÙŠÙ„Ùƒ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø¶ÙŠØ©.
Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ:
1. ÙÙ‡Ù… Ø§Ù„Ù‚Ø¶ÙŠØ© Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø¹Ø§Ù…
2. Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
3. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ:
{text}

Ø§Ù„Ø³Ø¤Ø§Ù„/Ø§Ù„ØªÙˆØ¬ÙŠÙ‡:
{prompt}"""

    llama_prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ØªØ¹Ù…Ù„ Ù…Ø¹ Ø²Ù…ÙŠÙ„Ùƒ.
Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ:
1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù„Ù„Ù‚Ø¶ÙŠØ©
2. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†
3. Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©

Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ:
{text}

Ø§Ù„Ø³Ø¤Ø§Ù„/Ø§Ù„ØªÙˆØ¬ÙŠÙ‡:
{prompt}"""

    return {
        'gemini': gemini_prompt,
        'llama': llama_prompt
    }

@chat_bp.route('/chat')
def chat_page():
    """Ø¹Ø±Ø¶ ØµÙØ­Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    return render_template('chat/chat.html')

@chat_bp.route('/api/chat', methods=['POST'])
def chat():
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    data = request.json
    message = data.get('message', '')
    stage = data.get('stage', 'initial')
    
    if not models:
        return jsonify({
            "status": "error",
            "message": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø©"
        }), 400
    
    try:
        prompts = get_chat_prompt(stage, message)
        responses = []
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Gemini
        if 'gemini' in models:
            try:
                gemini_response = models['gemini'].generate_content(prompts['gemini'])
                print(gemini_response)  # Ø·Ø¨Ø§Ø¹Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
                print(dir(gemini_response))  # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„ÙƒØ§Ø¦Ù†
                if gemini_response and hasattr(gemini_response, 'text'):
                    responses.append(f"ğŸ¤– Gemini ÙŠÙ‚ÙˆÙ„:\n{gemini_response.text}\n")
                elif hasattr(gemini_response, 'content'):
                    responses.append(f"ğŸ¤– Gemini ÙŠÙ‚ÙˆÙ„:\n{gemini_response.content}\n")
                else:
                    responses.append("ğŸ¤– Gemini Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† ØªÙ‚Ø¯ÙŠÙ… Ø±Ø¯.")
            except Exception as e:
                responses.append(f"ğŸ¤– Gemini Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Llama
        if 'llama' in models:
            try:
                completion = models['llama'].chat.completions.create(
                    messages=[{
                        "role": "user",
                        "content": prompts['llama']
                    }],
                    model="llama-3.3-70b-versatile",
                    temperature=1,
                    max_tokens=1024,
                    top_p=1,
                    stream=False,
                    stop=None
                )
                responses.append(f"ğŸ¤– Llama ÙŠÙ‚ÙˆÙ„:\n{completion['choices'][0]['message']['content']}\n")
            except Exception as e:
                responses.append(f"ğŸ¤– Llama Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
        
        if responses:
            combined_response = "\n".join(responses)
            return jsonify({
                "status": "success",
                "result": combined_response
            })
        else:
            return jsonify({
                "status": "error",
                "message": "Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù† Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬"
            }), 500
            
    except Exception as e:
        print(f"Error in chat: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500